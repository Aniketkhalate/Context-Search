{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FhShtKp56Li"
      },
      "outputs": [],
      "source": [
        "# We are going to use phi3 for our basic RAG implementation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing various packages\n",
        "\n",
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install gpt4all\n",
        "!pip install pypdf\n",
        "!pip install chromadb\n",
        "!pip install colab-xterm"
      ],
      "metadata": {
        "id": "HB_GsMuD6nWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "pRny5bKcBSl4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the terminal in colab to run and install ollama and run the Phi3 model\n",
        "%xterm"
      ],
      "metadata": {
        "id": "yND-5PseBaiL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are going to use Phi3\n",
        "\n",
        "from langchain_community.llms import Ollama\n",
        "llm = Ollama(model=\"phi3\")"
      ],
      "metadata": {
        "id": "ChZuhu6sAvFX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the embeddings that is going to be used to embed both the user input and documents.\n",
        "\n",
        "def get_embeddings():\n",
        "  from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "  embedding = GPT4AllEmbeddings(\n",
        "      model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
        "  )\n",
        "  return embedding"
      ],
      "metadata": {
        "id": "jZAsNKqp6nfW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the document of interest for splitting and indexing\n",
        "\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "loader = PyPDFLoader(r\"/content/research_paper_1.pdf\")\n",
        "pages = loader.load()\n",
        "print(len(pages)) #This should match the number of pages in the document"
      ],
      "metadata": {
        "id": "2RtAsLHa6nb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e53f38f-cfa1-4a07-bc69-ce78a7b7bca1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spilt the document and Get chunks\n",
        "\n",
        "def chunk(docs):\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "      separators=[\"\\n\\n\", \"\\n\", \"(?<=\\. )\", \" \", \"\"],\n",
        "      chunk_size= 1000,\n",
        "      chunk_overlap= 150,\n",
        "      length_function= len\n",
        "  )\n",
        "  chunks = text_splitter.split_documents(docs)\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "-Qb8Mas26niy"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks_of_pages = chunk(pages)\n",
        "print(len(chunks_of_pages))"
      ],
      "metadata": {
        "id": "NJIml1Qy6nml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3c12179-bdd0-4940-ec2a-f95a72ac1830"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a database to store the index and documents\n",
        "\n",
        "def create_chromadb(chunks, persist_directory):\n",
        "\n",
        "  from langchain.vectorstores import Chroma\n",
        "\n",
        "  db = Chroma.from_documents(\n",
        "  documents= chunks,\n",
        "  embedding= get_embeddings(),\n",
        "  persist_directory= persist_directory,\n",
        "  )\n",
        "  return db"
      ],
      "metadata": {
        "id": "btky3x9K6npn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pass the directory of folder to store the database\n",
        "\n",
        "persist_directory = \"/content/chroma\"\n",
        "embedded_database = create_chromadb(chunks_of_pages, persist_directory)\n",
        "print(embedded_database._collection.count()) #This should be equal to the number of chunks created earlier"
      ],
      "metadata": {
        "id": "LrjZlfgU6nsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19d529dd-ca24-4696-a26f-f1b553c5f6ab"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: 100%|██████████| 45.9M/45.9M [00:00<00:00, 126MiB/s]\n",
            "Verifying: 100%|██████████| 45.9M/45.9M [00:00<00:00, 556MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the content of documents separated by new line character\n",
        "\n",
        "def get_context(docs):\n",
        "  return \"\\n\\n---\\n\\n\".join(doc.page_content for doc in docs)"
      ],
      "metadata": {
        "id": "nnOgIiS16nwA"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a prompt template which takes in the content of documents as context and user query.\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "PROMPT_TEMPLATE= \"\"\" \"You are a smart AI assisstant. You have access to this information {context}. Based ONLY on the the given information answer the question {question} \"\"\"\n",
        "prompt= prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=PROMPT_TEMPLATE,\n",
        ")"
      ],
      "metadata": {
        "id": "Ng6uvsWdAEZe"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the earlier created database as retriever\n",
        "\n",
        "retriever = embedded_database.as_retriever(search_type= \"similarity\", search_kwargs= {'k': 5})"
      ],
      "metadata": {
        "id": "dw5X71TOAElL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a chain which takes in the user query and will produce the output\n",
        "\n",
        "rag_chain = (\n",
        "    {'context': retriever | get_context, 'question': RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "xwEtfW5bAEqj"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is SuperGLUE?\"\n",
        "res = rag_chain.invoke(question)"
      ],
      "metadata": {
        "id": "LgHFlGu_Ds7G"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "AB38kZp1D5kW",
        "outputId": "33511386-3048-4ae0-9f54-d549257df410"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"SuperGLUE is a standardized collection of datasets designed for evaluating and comparing performance across various NLP tasks, including Natural Language Inference (NLI), Winograd Schema Challenge (WSC), Multi-RC, SAT, WUP (Winograd Syntax Challenge), MNLI (Multi-Genre Natural Language Inference), MultiNLI (a large crowd-sourced dataset for NLI tasks), and others. It aims to provide a common framework for assessing the capabilities of different models like GPT-3, BERT variants, and their fine-tuned versions on these diverse challenges in natural language processing. The performance trends shown in Figure 3.8 indicate that model size and the number of context examples have a positive impact on SuperGLUE task performances, with few-shot learning capabilities also being significant as demonstrated by GPT-3's results.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is zero shot learner?\"\n",
        "res2 = rag_chain.invoke(question)"
      ],
      "metadata": {
        "id": "Kp6HT49lFr5W"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "f0Omq-dZGdmC",
        "outputId": "d40aaa3f-d215-470c-bea1-b590df9de9ba"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A zero-shot learner, in the context of natural language processing and machine learning, refers to a model that can perform tasks or understand concepts without having been explicitly trained (fine-tuned) with examples for those specific tasks. It relies solely on its pre-training knowledge acquired from a broad distribution of related tasks or data. This approach is most analogous to humans communicating instructions to the model using natural language descriptions, expecting it to apply general understanding and reasoning skills without direct task demonstrations. For instance, in Figure 2.1 mentioned in your text, zero-shot learning involves providing the model with a natural language instruction describing the task (and no examples) at test time. This method offers maximum convenience and robustness by avoiding spurious correlations but is challenging due to its reliance on generalizing from pre-training data without any gradient updates or fine-tuning during testing.\\n\\nZero-shot learning demonstrates promising results in various settings, including zero-shot (no examples given), one-shot (one example provided), and even some few-shot scenarios (a small number of examples). It showcases the potential for models like GPT-3 to quickly adapt or reason on new tasks without needing task-specific fine-tuning. However, while achieving impressive results in certain benchmarks compared to state-of-the-art finetuned models, zero-shot learners often face challenges due to the absence of direct examples during testing, emphasizing the importance and ongoing research efforts towards improving this capability.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFnai_6eGi_Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}